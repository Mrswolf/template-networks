{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "650fb3be-6aab-4f7d-97b5-30ca95b36bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy, time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.signal import sosfiltfilt\n",
    "from sklearn.pipeline import make_pipeline, clone\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "from brainda.datasets import Nakanishi2015, Wang2016, BETA\n",
    "from brainda.paradigms import SSVEP\n",
    "from brainda.algorithms.utils.model_selection import (\n",
    "    set_random_seeds,\n",
    "    generate_loo_indices, match_loo_indices)\n",
    "from brainda.algorithms.decomposition import (\n",
    "    generate_filterbank, generate_cca_references)\n",
    "from brainda.algorithms.deep_learning import EEGNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import skorch\n",
    "\n",
    "from skorch.helper import predefined_split\n",
    "\n",
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515cafe5-5671-441b-9fa8-d60817f5e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file(\n",
    "    dataset, model_name, channels, srate, duration, events, \n",
    "    preprocess=None, \n",
    "    n_bands=None,\n",
    "    augment=False, loo=False, fixed_dtn_template=False):\n",
    "    file = \"{:s}-{:s}-{ch:d}-{srate:d}-{nt:d}-{event:d}\".format(\n",
    "        dataset.dataset_code,\n",
    "        model_name,\n",
    "        ch=len(channels), \n",
    "        srate=srate, \n",
    "        nt=int(duration*srate),\n",
    "        event=len(events))\n",
    "    if n_bands is not None:\n",
    "        file += '-{:d}'.format(n_bands)\n",
    "    if preprocess is not None:\n",
    "        file += '-{:s}'.format(preprocess)\n",
    "    if augment:\n",
    "        file += '-augment'\n",
    "    if fixed_dtn_template:\n",
    "        file += '-fixed'\n",
    "    if loo:\n",
    "        file += '-loo'\n",
    "    file += '.joblib'\n",
    "    return file\n",
    "\n",
    "def make_dl_model(\n",
    "        model_name, n_channels, n_samples, n_classes, \n",
    "        Yf=None):\n",
    "    set_random_seeds(64)\n",
    "    if model_name == 'eegnet-ssvep':\n",
    "        model = EEGNet(\n",
    "            n_channels, n_samples, n_classes,\n",
    "            time_kernel=(96, (1, n_samples), (1, 1)), \n",
    "            D=1,\n",
    "            separa_kernel=(96, (1, 16), (1, 1)),\n",
    "            dropout_rate=0.5,\n",
    "            fc_norm_rate=1)\n",
    "    elif model_name == 'dtn':\n",
    "        model = DTN(\n",
    "            3, 120, n_channels, n_samples, n_classes, \n",
    "            band_kernel=9, pooling_kernel=2, \n",
    "            dropout=0.95, momentum=0.1)\n",
    "    elif model_name == 'ftn':\n",
    "        model = FTN(\n",
    "            3, 120, n_channels, n_samples, n_classes, Yf, \n",
    "            band_kernel=9, pooling_kernel=2,\n",
    "            dropout=0.95)\n",
    "    return model\n",
    "\n",
    "def generate_dl_parameters(dataset, model_name):\n",
    "    batch_size = 256\n",
    "    max_epochs = 600\n",
    "    lr = 1e-3\n",
    "    if model_name == 'eegnet-ssvep':\n",
    "        lr = 1e-2\n",
    "    return lr, batch_size, max_epochs\n",
    "\n",
    "def generate_dl_ft_parameters(dataset, model_name):\n",
    "    max_epochs = 100\n",
    "    lr = 1e-3\n",
    "    \n",
    "    batch_sizes = {\n",
    "        'nakanishi2015': 32,\n",
    "        'wang2016': 32,\n",
    "        'beta': 32,\n",
    "    }\n",
    "    \n",
    "    batch_size = batch_sizes[dataset.dataset_code]\n",
    "    \n",
    "    if model_name == 'eegnet-ssvep':\n",
    "        lr = 1e-2\n",
    "    \n",
    "    return lr, batch_size, max_epochs\n",
    "\n",
    "def exchange(X, n_trials, seeds=253, dropout=0):\n",
    "    np.random.seed(seeds)\n",
    "    ind1 = np.random.randint(0, high=len(X), size=(n_trials, X.shape[1]))\n",
    "    ind2 = np.tile(np.arange(X.shape[1]), (n_trials, 1))\n",
    "    newX = X[ind1, ind2, ...]\n",
    "    if dropout != 0:\n",
    "        newX *= np.random.binomial(np.ones(X.shape[1], dtype=np.int), 1-dropout)[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "    return newX\n",
    "    \n",
    "def generate_augment_data(X, y, filterbank, n_trials_per_class=0, seeds=253, dropout=0):\n",
    "    if n_trials_per_class != 0:\n",
    "        filterX = np.stack(\n",
    "            [sosfiltfilt(filterbank[i], np.copy(X), axis=-1) for i in range(len(filterbank))], axis=1)\n",
    "        resX = X - np.sum(filterX, 1)\n",
    "        filterX = np.concatenate([filterX, resX[:, np.newaxis, ...]], axis=1)\n",
    "        labels = np.unique(y)\n",
    "        augX, augY = [], []\n",
    "        for label in labels:\n",
    "            newX = np.sum(exchange(filterX[y==label], n_trials_per_class, seeds=seeds, dropout=dropout), axis=1)\n",
    "            augX.append(newX)\n",
    "            augY.append(np.ones(len(newX))*label)\n",
    "        augX = np.concatenate(augX, axis=0)\n",
    "        augY = np.concatenate(augY, axis=0)\n",
    "    else:\n",
    "        augX = None\n",
    "        augY = None\n",
    "    return augX, augY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8217526-276b-44f2-bb97-333adf18f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available GPU devices: 1\n",
      "Current pytorch device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = torch.device(\"cuda:{:d}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "print(\"Total available GPU devices: {}\".format(torch.cuda.device_count()))\n",
    "print(\"Current pytorch device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b42fddd7-43c7-46ac-8f16-eaf4203bb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    Nakanishi2015(), \n",
    "    Wang2016(), \n",
    "    BETA()\n",
    "]\n",
    "delays = [\n",
    "    0.135, \n",
    "    0.14, \n",
    "    0.13\n",
    "]\n",
    "channels = [\n",
    "    ['PO7', 'PO3', 'POZ', 'PO4', 'PO8', 'O1', 'OZ', 'O2'],\n",
    "    ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2'],\n",
    "    ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2']\n",
    "]\n",
    "\n",
    "srate = 250\n",
    "durations = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "def data_hook(X, y, meta, caches):\n",
    "    filterbank = generate_filterbank([[8, 90]], [[6, 95]], srate, order=4, rp=1)\n",
    "    X = sosfiltfilt(filterbank[0], X, axis=-1)\n",
    "    return X, y, meta, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3ab95c-b6bf-4ca7-ac29-60619b0b7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dtype, y_dtype = torch.float, torch.long\n",
    "\n",
    "models = ['eegnet-ssvep', 'ftn','dtn']\n",
    "\n",
    "n_bands = 3\n",
    "n_harmonics = 5\n",
    "\n",
    "force_update = False\n",
    "save_folder = 'neural_networks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9dc54-df70-480d-86bb-fbd6f0c02ce7",
   "metadata": {},
   "source": [
    "## within-subject classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5078ce0-83be-42b6-a086-8967efe1dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: nakanishi2015 Size: (1800, 8, 274)\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-50-12-3.joblib acc:0.4844 ft_acc:0.6428 ft_aug_acc:0.6589\n",
      "neural_networks/nakanishi2015-ftn-8-250-50-12-3.joblib acc:0.4894 ft_acc:0.7233 ft_aug_acc:0.7333\n",
      "neural_networks/nakanishi2015-dtn-8-250-50-12-3.joblib acc:0.4822 ft_acc:0.7128 ft_aug_acc:0.7311\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-75-12-3.joblib acc:0.5761 ft_acc:0.7294 ft_aug_acc:0.7428\n",
      "neural_networks/nakanishi2015-ftn-8-250-75-12-3.joblib acc:0.6183 ft_acc:0.7989 ft_aug_acc:0.8144\n",
      "neural_networks/nakanishi2015-dtn-8-250-75-12-3.joblib acc:0.5778 ft_acc:0.8011 ft_aug_acc:0.8200\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-100-12-3.joblib acc:0.6511 ft_acc:0.7828 ft_aug_acc:0.8133\n",
      "neural_networks/nakanishi2015-ftn-8-250-100-12-3.joblib acc:0.6861 ft_acc:0.8394 ft_aug_acc:0.8528\n",
      "neural_networks/nakanishi2015-dtn-8-250-100-12-3.joblib acc:0.7000 ft_acc:0.8772 ft_aug_acc:0.8811\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-125-12-3.joblib acc:0.6856 ft_acc:0.8100 ft_aug_acc:0.8300\n",
      "neural_networks/nakanishi2015-ftn-8-250-125-12-3.joblib acc:0.7333 ft_acc:0.8500 ft_aug_acc:0.8700\n",
      "neural_networks/nakanishi2015-dtn-8-250-125-12-3.joblib acc:0.7550 ft_acc:0.9017 ft_aug_acc:0.9033\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-150-12-3.joblib acc:0.7189 ft_acc:0.8311 ft_aug_acc:0.8478\n",
      "neural_networks/nakanishi2015-ftn-8-250-150-12-3.joblib acc:0.7633 ft_acc:0.8606 ft_aug_acc:0.8783\n",
      "neural_networks/nakanishi2015-dtn-8-250-150-12-3.joblib acc:0.7661 ft_acc:0.9150 ft_aug_acc:0.9228\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-175-12-3.joblib acc:0.7311 ft_acc:0.8456 ft_aug_acc:0.8622\n",
      "neural_networks/nakanishi2015-ftn-8-250-175-12-3.joblib acc:0.7878 ft_acc:0.8794 ft_aug_acc:0.8939\n",
      "neural_networks/nakanishi2015-dtn-8-250-175-12-3.joblib acc:0.8139 ft_acc:0.9306 ft_aug_acc:0.9333\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-200-12-3.joblib acc:0.7894 ft_acc:0.8656 ft_aug_acc:0.8894\n",
      "neural_networks/nakanishi2015-ftn-8-250-200-12-3.joblib acc:0.8322 ft_acc:0.9083 ft_aug_acc:0.9139\n",
      "neural_networks/nakanishi2015-dtn-8-250-200-12-3.joblib acc:0.8428 ft_acc:0.9467 ft_aug_acc:0.9522\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-225-12-3.joblib acc:0.8217 ft_acc:0.8933 ft_aug_acc:0.9056\n",
      "neural_networks/nakanishi2015-ftn-8-250-225-12-3.joblib acc:0.8378 ft_acc:0.9167 ft_aug_acc:0.9233\n",
      "neural_networks/nakanishi2015-dtn-8-250-225-12-3.joblib acc:0.8533 ft_acc:0.9611 ft_aug_acc:0.9628\n",
      "neural_networks/nakanishi2015-eegnet-ssvep-8-250-250-12-3.joblib acc:0.8383 ft_acc:0.9022 ft_aug_acc:0.9161\n",
      "neural_networks/nakanishi2015-ftn-8-250-250-12-3.joblib acc:0.8728 ft_acc:0.9300 ft_aug_acc:0.9367\n",
      "neural_networks/nakanishi2015-dtn-8-250-250-12-3.joblib acc:0.8850 ft_acc:0.9594 ft_aug_acc:0.9650\n",
      "Dataset: wang2016 Size: (8400, 9, 275)\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-50-40-3.joblib acc:0.1873 ft_acc:0.3020 ft_aug_acc:0.2976\n",
      "neural_networks/wang2016-ftn-9-250-50-40-3.joblib acc:0.2462 ft_acc:0.4924 ft_aug_acc:0.5285\n",
      "neural_networks/wang2016-dtn-9-250-50-40-3.joblib acc:0.2350 ft_acc:0.4807 ft_aug_acc:0.5136\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-75-40-3.joblib acc:0.2585 ft_acc:0.3880 ft_aug_acc:0.3798\n",
      "neural_networks/wang2016-ftn-9-250-75-40-3.joblib acc:0.3693 ft_acc:0.6762 ft_aug_acc:0.7031\n",
      "neural_networks/wang2016-dtn-9-250-75-40-3.joblib acc:0.3602 ft_acc:0.6563 ft_aug_acc:0.6800\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-100-40-3.joblib acc:0.3350 ft_acc:0.4639 ft_aug_acc:0.4551\n",
      "neural_networks/wang2016-ftn-9-250-100-40-3.joblib acc:0.4601 ft_acc:0.7474 ft_aug_acc:0.7674\n",
      "neural_networks/wang2016-dtn-9-250-100-40-3.joblib acc:0.4532 ft_acc:0.7306 ft_aug_acc:0.7556\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-125-40-3.joblib acc:0.4036 ft_acc:0.5264 ft_aug_acc:0.5289\n",
      "neural_networks/wang2016-ftn-9-250-125-40-3.joblib acc:0.5282 ft_acc:0.7885 ft_aug_acc:0.8099\n",
      "neural_networks/wang2016-dtn-9-250-125-40-3.joblib acc:0.5262 ft_acc:0.7833 ft_aug_acc:0.7969\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-150-40-3.joblib acc:0.4769 ft_acc:0.6140 ft_aug_acc:0.6139\n",
      "neural_networks/wang2016-ftn-9-250-150-40-3.joblib acc:0.6055 ft_acc:0.8280 ft_aug_acc:0.8411\n",
      "neural_networks/wang2016-dtn-9-250-150-40-3.joblib acc:0.5927 ft_acc:0.8231 ft_aug_acc:0.8335\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-175-40-3.joblib acc:0.5663 ft_acc:0.6973 ft_aug_acc:0.7037\n",
      "neural_networks/wang2016-ftn-9-250-175-40-3.joblib acc:0.6800 ft_acc:0.8736 ft_aug_acc:0.8807\n",
      "neural_networks/wang2016-dtn-9-250-175-40-3.joblib acc:0.6752 ft_acc:0.8638 ft_aug_acc:0.8813\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-200-40-3.joblib acc:0.6186 ft_acc:0.7464 ft_aug_acc:0.7558\n",
      "neural_networks/wang2016-ftn-9-250-200-40-3.joblib acc:0.7464 ft_acc:0.9000 ft_aug_acc:0.9079\n",
      "neural_networks/wang2016-dtn-9-250-200-40-3.joblib acc:0.7387 ft_acc:0.9027 ft_aug_acc:0.9098\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-225-40-3.joblib acc:0.6630 ft_acc:0.7886 ft_aug_acc:0.8030\n",
      "neural_networks/wang2016-ftn-9-250-225-40-3.joblib acc:0.7880 ft_acc:0.9262 ft_aug_acc:0.9323\n",
      "neural_networks/wang2016-dtn-9-250-225-40-3.joblib acc:0.7885 ft_acc:0.9245 ft_aug_acc:0.9314\n",
      "neural_networks/wang2016-eegnet-ssvep-9-250-250-40-3.joblib acc:0.6940 ft_acc:0.8004 ft_aug_acc:0.8161\n",
      "neural_networks/wang2016-ftn-9-250-250-40-3.joblib acc:0.8104 ft_acc:0.9319 ft_aug_acc:0.9396\n",
      "neural_networks/wang2016-dtn-9-250-250-40-3.joblib acc:0.8056 ft_acc:0.9275 ft_aug_acc:0.9370\n",
      "Dataset: beta Size: (11200, 9, 275)\n",
      "neural_networks/beta-eegnet-ssvep-9-250-50-40-3.joblib acc:0.1678 ft_acc:0.2038 ft_aug_acc:0.1820\n",
      "neural_networks/beta-ftn-9-250-50-40-3.joblib acc:0.1939 ft_acc:0.3613 ft_aug_acc:0.4093\n",
      "neural_networks/beta-dtn-9-250-50-40-3.joblib acc:0.1980 ft_acc:0.3040 ft_aug_acc:0.3592\n",
      "neural_networks/beta-eegnet-ssvep-9-250-75-40-3.joblib acc:0.2229 ft_acc:0.2722 ft_aug_acc:0.2659\n",
      "neural_networks/beta-ftn-9-250-75-40-3.joblib acc:0.2925 ft_acc:0.5337 ft_aug_acc:0.5799\n",
      "neural_networks/beta-dtn-9-250-75-40-3.joblib acc:0.2861 ft_acc:0.4605 ft_aug_acc:0.5183\n",
      "neural_networks/beta-eegnet-ssvep-9-250-100-40-3.joblib acc:0.2921 ft_acc:0.3457 ft_aug_acc:0.3576\n",
      "neural_networks/beta-ftn-9-250-100-40-3.joblib acc:0.3715 ft_acc:0.6293 ft_aug_acc:0.6645\n",
      "neural_networks/beta-dtn-9-250-100-40-3.joblib acc:0.3600 ft_acc:0.5557 ft_aug_acc:0.6136\n",
      "neural_networks/beta-eegnet-ssvep-9-250-125-40-3.joblib acc:0.3341 ft_acc:0.3822 ft_aug_acc:0.3811\n",
      "neural_networks/beta-ftn-9-250-125-40-3.joblib acc:0.4145 ft_acc:0.6624 ft_aug_acc:0.6941\n",
      "neural_networks/beta-dtn-9-250-125-40-3.joblib acc:0.4092 ft_acc:0.6011 ft_aug_acc:0.6564\n",
      "neural_networks/beta-eegnet-ssvep-9-250-150-40-3.joblib acc:0.3773 ft_acc:0.4237 ft_aug_acc:0.4407\n",
      "neural_networks/beta-ftn-9-250-150-40-3.joblib acc:0.4704 ft_acc:0.7061 ft_aug_acc:0.7337\n",
      "neural_networks/beta-dtn-9-250-150-40-3.joblib acc:0.4615 ft_acc:0.6659 ft_aug_acc:0.6986\n",
      "neural_networks/beta-eegnet-ssvep-9-250-175-40-3.joblib acc:0.4209 ft_acc:0.4658 ft_aug_acc:0.4920\n",
      "neural_networks/beta-ftn-9-250-175-40-3.joblib acc:0.5097 ft_acc:0.7372 ft_aug_acc:0.7612\n",
      "neural_networks/beta-dtn-9-250-175-40-3.joblib acc:0.5114 ft_acc:0.7000 ft_aug_acc:0.7323\n",
      "neural_networks/beta-eegnet-ssvep-9-250-200-40-3.joblib acc:0.4584 ft_acc:0.5129 ft_aug_acc:0.5372\n",
      "neural_networks/beta-ftn-9-250-200-40-3.joblib acc:0.5458 ft_acc:0.7534 ft_aug_acc:0.7747\n",
      "neural_networks/beta-dtn-9-250-200-40-3.joblib acc:0.5463 ft_acc:0.7279 ft_aug_acc:0.7612\n",
      "neural_networks/beta-eegnet-ssvep-9-250-225-40-3.joblib acc:0.4888 ft_acc:0.5408 ft_aug_acc:0.5728\n",
      "neural_networks/beta-ftn-9-250-225-40-3.joblib acc:0.5709 ft_acc:0.7728 ft_aug_acc:0.7914\n",
      "neural_networks/beta-dtn-9-250-225-40-3.joblib acc:0.5769 ft_acc:0.7472 ft_aug_acc:0.7788\n",
      "neural_networks/beta-eegnet-ssvep-9-250-250-40-3.joblib acc:0.5132 ft_acc:0.5685 ft_aug_acc:0.5962\n",
      "neural_networks/beta-ftn-9-250-250-40-3.joblib acc:0.5974 ft_acc:0.7829 ft_aug_acc:0.7956\n",
      "neural_networks/beta-dtn-9-250-250-40-3.joblib acc:0.6013 ft_acc:0.7630 ft_aug_acc:0.7851\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_channels, delay in zip(datasets, channels, delays):\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    dataset_events = sorted(list(dataset.events.keys()))\n",
    "    freqs = [dataset.get_freq(event) for event in dataset_events]\n",
    "    phases = [dataset.get_phase(event) for event in dataset_events]\n",
    "    \n",
    "    X, y, meta = get_ssvep_data(\n",
    "        dataset, srate, dataset_channels, 1.1, dataset_events, \n",
    "        delay=delay, \n",
    "        data_hook=data_hook)\n",
    "    labels = np.unique(y)\n",
    "    Yf = generate_cca_references(\n",
    "        freqs, srate, 1.1, \n",
    "        phases=None, \n",
    "        n_harmonics=n_harmonics)\n",
    "    print(\"Dataset: {} Size: {}\".format(dataset.dataset_code, X.shape))\n",
    "    _, n_channels, n_samples = X.shape\n",
    "    n_classes = len(labels)\n",
    "    \n",
    "    indices = joblib.load(\n",
    "        \"indices/{:s}-loo-{:d}class-indices.joblib\".format(\n",
    "        dataset.dataset_code, n_classes))['indices']\n",
    "    loo = len(indices[1][dataset_events[0]])\n",
    "    \n",
    "    min_f, max_f = np.min(freqs), np.max(freqs)\n",
    "    wp = [[min_f*i, max_f*i] for i in range(1, n_harmonics+1)]\n",
    "    ws= [[min_f*i-2, max_f*i+2] for i in range(1, n_harmonics+1)]\n",
    "    aug_filterbank = generate_filterbank(wp, ws, srate, order=4, rp=1)\n",
    "    \n",
    "    for duration in durations:\n",
    "        for model_name in models:\n",
    "            file_name = make_file(\n",
    "                dataset, model_name, dataset_channels, srate, duration, dataset_events, \n",
    "                n_bands=n_bands)\n",
    "            save_file = os.path.join(save_folder, file_name)\n",
    "            if not force_update and os.path.exists(save_file):\n",
    "                scores = joblib.load(save_file)\n",
    "                global_sub_accs = scores['global_sub_accs']\n",
    "                ft_sub_accs = scores['ft_sub_accs']\n",
    "                ft_aug_sub_accs = scores['ft_aug_sub_accs']\n",
    "                print(\"{:s} acc:{:.4f} ft_acc:{:.4f} ft_aug_acc:{:.4f}\".format(\n",
    "                    save_file, np.mean(global_sub_accs), np.mean(ft_sub_accs), np.mean(ft_aug_sub_accs)))\n",
    "                continue  \n",
    "                \n",
    "            set_random_seeds(42)\n",
    "            loo_global_accs = []\n",
    "            loo_global_model_states = []\n",
    "            loo_fine_tuning_accs = []\n",
    "            loo_fine_tuning_accs_aug = []\n",
    "\n",
    "            for k in range(loo):\n",
    "                filterX, filterY = np.copy(X[..., :int(srate*duration)]), np.copy(y)\n",
    "                filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "\n",
    "                train_ind, validate_ind, test_ind = match_loo_indices(\n",
    "                    k, meta, indices)\n",
    "                trainX, trainY, trainMeta = filterX[train_ind], filterY[train_ind], meta.iloc[train_ind]\n",
    "                validateX, validateY, validateMeta = filterX[validate_ind], filterY[validate_ind], meta.iloc[validate_ind]\n",
    "                testX, testY, testMeta = filterX[test_ind], filterY[test_ind], meta.iloc[test_ind]\n",
    "\n",
    "                trainX, validateX, testX = generate_tensors(\n",
    "                    trainX, validateX, testX, dtype=x_dtype)\n",
    "                trainY, validateY, testY = generate_tensors(\n",
    "                    trainY, validateY, testY, dtype=y_dtype) \n",
    "\n",
    "                net = make_dl_model(\n",
    "                    model_name, n_channels, int(srate*duration), n_classes,\n",
    "                    Yf=Yf[..., :int(srate*duration)])\n",
    "                \n",
    "                lr, batch_size, max_epochs = generate_dl_parameters(\n",
    "                    dataset, model_name)\n",
    "                net.device = device\n",
    "                net.lr = lr\n",
    "                net.max_epochs = max_epochs\n",
    "                net.batch_size = batch_size\n",
    "                net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "                net.verbose=False\n",
    "\n",
    "                net.train_split = predefined_split(\n",
    "                    skorch.dataset.Dataset(\n",
    "                        {'X': validateX}, validateY))\n",
    "\n",
    "                if model_name == 'dtn':\n",
    "                    net = net.fit({'X': trainX, 'y': trainY}, y=trainY)\n",
    "                else:\n",
    "                    net = net.fit(\n",
    "                        {'X': trainX}, \n",
    "                        y=trainY)\n",
    "\n",
    "                loo_global_model_states.append(\n",
    "                    copy.deepcopy(net.module.state_dict()))\n",
    "                \n",
    "                ## testing\n",
    "                sub_accs = []\n",
    "                for sub_id in dataset.subjects:\n",
    "                    sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "                    pred_labels = net.predict({'X': testX[sub_test_mask]})\n",
    "                    true_labels = testY[sub_test_mask].numpy()\n",
    "                    sub_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "                    sub_accs.append(sub_acc)\n",
    "                loo_global_accs.append(sub_accs)\n",
    "\n",
    "                ## fine-tuning\n",
    "                sub_accs = []\n",
    "                sub_accs_aug = []\n",
    "                for sub_id in dataset.subjects:\n",
    "                    sub_train_mask = (trainMeta['subject']==sub_id).to_numpy()\n",
    "                    sub_valid_mask = (validateMeta['subject']==sub_id).to_numpy()\n",
    "                    sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "\n",
    "                    sub_trainX, sub_trainY = trainX[sub_train_mask], trainY[sub_train_mask]\n",
    "                    sub_validateX, sub_validateY = validateX[sub_valid_mask], validateY[sub_valid_mask]\n",
    "                    sub_testX, sub_testY = testX[sub_test_mask], testY[sub_test_mask]    \n",
    "\n",
    "                    lr, batch_size, max_epochs = generate_dl_ft_parameters(\n",
    "                        dataset, model_name)\n",
    "\n",
    "                    net = make_dl_model(\n",
    "                        model_name, n_channels, int(srate*duration), n_classes,\n",
    "                        Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "                    net.device = device\n",
    "                    net.lr = lr\n",
    "                    net.max_epochs = max_epochs\n",
    "                    net.batch_size = batch_size\n",
    "                    net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "                    net.verbose=False\n",
    "                    net.module.load_state_dict(\n",
    "                        copy.deepcopy(loo_global_model_states[k]))\n",
    "\n",
    "                    net.train_split = predefined_split(\n",
    "                        skorch.dataset.Dataset(\n",
    "                            {'X': sub_validateX}, sub_validateY))\n",
    "\n",
    "                    if model_name == 'dtn':\n",
    "                        net = net.fit({'X': sub_trainX, 'y': sub_trainY}, y=sub_trainY)\n",
    "                    else:\n",
    "                        net = net.fit(\n",
    "                            {'X': sub_trainX}, \n",
    "                            y=sub_trainY)\n",
    "\n",
    "                    pred_labels = net.predict({'X': sub_testX})\n",
    "                    true_labels = sub_testY.numpy()\n",
    "                    sub_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "                    sub_accs.append(sub_acc)\n",
    "                    \n",
    "                    # augment training and validation data\n",
    "                    # the augmented data used for validation while the original training and validation data are combined for training\n",
    "                    sub_aug_trainX, sub_aug_trainY = generate_augment_data(\n",
    "                        np.concatenate([sub_trainX.numpy(), sub_validateX.numpy()], axis=0),\n",
    "                        np.concatenate([sub_trainY.numpy(), sub_validateY.numpy()], axis=0), aug_filterbank,\n",
    "                        n_trials_per_class=20)\n",
    "                    sub_aug_trainX = generate_tensors(sub_aug_trainX, dtype=x_dtype)\n",
    "                    sub_aug_trainY = generate_tensors(sub_aug_trainY, dtype=y_dtype)\n",
    "                    sub_trainX = torch.cat([sub_trainX, sub_validateX], 0)\n",
    "                    sub_trainY = torch.cat([sub_trainY, sub_validateY], 0) \n",
    "\n",
    "                    sub_validateX = sub_aug_trainX\n",
    "                    sub_validateY = sub_aug_trainY \n",
    "\n",
    "                    lr, batch_size, max_epochs = generate_dl_ft_parameters(\n",
    "                        dataset, model_name)\n",
    "\n",
    "                    net = make_dl_model(\n",
    "                        model_name, n_channels, int(srate*duration), n_classes,\n",
    "                        Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "                    net.device = device\n",
    "                    net.lr = lr\n",
    "                    net.max_epochs = max_epochs\n",
    "                    net.batch_size = batch_size\n",
    "                    net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "                    net.verbose=False\n",
    "                    net.module.load_state_dict(\n",
    "                        copy.deepcopy(loo_global_model_states[k]))\n",
    "\n",
    "                    net.train_split = predefined_split(\n",
    "                        skorch.dataset.Dataset(\n",
    "                            {'X': sub_validateX}, sub_validateY))\n",
    "\n",
    "                    if model_name == 'dtn':\n",
    "                        net = net.fit({'X': sub_trainX, 'y': sub_trainY}, y=sub_trainY)\n",
    "                    else:\n",
    "                        net = net.fit(\n",
    "                            {'X': sub_trainX}, \n",
    "                            y=sub_trainY)\n",
    "\n",
    "                    pred_labels = net.predict({'X': sub_testX})\n",
    "                    true_labels = sub_testY.numpy()\n",
    "                    sub_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "                    sub_accs_aug.append(sub_acc)\n",
    "                loo_fine_tuning_accs.append(sub_accs)\n",
    "                loo_fine_tuning_accs_aug.append(sub_accs_aug)\n",
    "            global_sub_accs = np.array(loo_global_accs).T\n",
    "            ft_sub_accs = np.array(loo_fine_tuning_accs).T\n",
    "            ft_aug_sub_accs = np.array(loo_fine_tuning_accs_aug).T\n",
    "            joblib.dump(\n",
    "                {\n",
    "                    'global_sub_accs': global_sub_accs, \n",
    "                    'ft_sub_accs': ft_sub_accs,\n",
    "                    'ft_aug_sub_accs': ft_aug_sub_accs\n",
    "                }, save_file)\n",
    "            torch.save(loo_global_model_states, save_file.replace('joblib', 'pt'))\n",
    "            print(\"{:s} acc:{:.4f} ft_acc:{:.4f} ft_aug_acc:{:.4f}\".format(\n",
    "                save_file, np.mean(global_sub_accs), np.mean(ft_sub_accs), np.mean(ft_aug_sub_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ecbbb-3879-42e4-aa57-a23adf1dd271",
   "metadata": {},
   "source": [
    "replace dtn templates with grand average templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46940378-e57e-4dd9-9f09-adf2a71a5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    Nakanishi2015(), \n",
    "    Wang2016(), \n",
    "    BETA()\n",
    "]\n",
    "delays = [\n",
    "    0.135, \n",
    "    0.14, \n",
    "    0.13\n",
    "]\n",
    "channels = [\n",
    "    ['PO7', 'PO3', 'POZ', 'PO4', 'PO8', 'O1', 'OZ', 'O2'],\n",
    "    ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2'],\n",
    "    ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2']\n",
    "]\n",
    "\n",
    "srate = 250\n",
    "durations = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "def data_hook(X, y, meta, caches):\n",
    "    filterbank = generate_filterbank([[8, 90]], [[6, 95]], srate, order=4, rp=1)\n",
    "    X = sosfiltfilt(filterbank[0], X, axis=-1)\n",
    "    return X, y, meta, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6660806a-8caa-44c4-b5cd-869d605229d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dtype, y_dtype = torch.float, torch.long\n",
    "\n",
    "models = ['dtn']\n",
    "\n",
    "n_bands = 3\n",
    "n_harmonics = 5\n",
    "\n",
    "force_update = False\n",
    "save_folder = 'neural_networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c02173-beb1-47e5-a44c-1b0d6b4feed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: nakanishi2015 Size: (1800, 8, 274)\n",
      "neural_networks/nakanishi2015-dtn-8-250-50-12-3-fixed.joblib acc:0.5811 ft_aug_acc:0.7272\n",
      "neural_networks/nakanishi2015-dtn-8-250-75-12-3-fixed.joblib acc:0.6961 ft_aug_acc:0.8161\n",
      "neural_networks/nakanishi2015-dtn-8-250-100-12-3-fixed.joblib acc:0.8011 ft_aug_acc:0.8800\n"
     ]
    }
   ],
   "source": [
    "for dataset, dataset_channels, delay in zip(datasets, channels, delays):\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    dataset_events = sorted(list(dataset.events.keys()))\n",
    "    freqs = [dataset.get_freq(event) for event in dataset_events]\n",
    "    phases = [dataset.get_phase(event) for event in dataset_events]\n",
    "    \n",
    "    X, y, meta = get_ssvep_data(\n",
    "        dataset, srate, dataset_channels, 1.1, dataset_events, \n",
    "        delay=delay, \n",
    "        data_hook=data_hook)\n",
    "    labels = np.unique(y)\n",
    "    Yf = generate_cca_references(\n",
    "        freqs, srate, 1.1, \n",
    "        phases=None, \n",
    "        n_harmonics=n_harmonics)\n",
    "    print(\"Dataset: {} Size: {}\".format(dataset.dataset_code, X.shape))\n",
    "    _, n_channels, n_samples = X.shape\n",
    "    n_classes = len(labels)\n",
    "    \n",
    "    indices = joblib.load(\n",
    "        \"indices/{:s}-loo-{:d}class-indices.joblib\".format(\n",
    "        dataset.dataset_code, n_classes))['indices']\n",
    "    loo = len(indices[1][dataset_events[0]])\n",
    "    \n",
    "    min_f, max_f = np.min(freqs), np.max(freqs)\n",
    "    wp = [[min_f*i, max_f*i] for i in range(1, n_harmonics+1)]\n",
    "    ws= [[min_f*i-2, max_f*i+2] for i in range(1, n_harmonics+1)]\n",
    "    aug_filterbank = generate_filterbank(wp, ws, srate, order=4, rp=1)\n",
    "    \n",
    "    for duration in durations:\n",
    "        for model_name in models:\n",
    "            model_file = make_file(\n",
    "                dataset, model_name, dataset_channels, srate, duration, dataset_events, \n",
    "                n_bands=n_bands)\n",
    "            model_file = os.path.join(save_folder, model_file)\n",
    "            loo_global_model_states = torch.load(\n",
    "                model_file.replace('.joblib', '.pt'), map_location='cpu')\n",
    "            \n",
    "            file_name = make_file(\n",
    "                dataset, model_name, dataset_channels, srate, duration, dataset_events, \n",
    "                n_bands=n_bands, fixed_dtn_template=True)\n",
    "            save_file = os.path.join(save_folder, file_name)\n",
    "            if not force_update and os.path.exists(save_file):\n",
    "                scores = joblib.load(save_file)\n",
    "                global_sub_accs_fixed = scores['global_sub_accs_fixed']\n",
    "                ft_aug_sub_accs_fixed = scores['ft_aug_sub_accs_fixed']\n",
    "                print(\"{:s} acc:{:.4f} ft_aug_acc:{:.4f}\".format(\n",
    "                    save_file, np.mean(global_sub_accs_fixed), np.mean(ft_aug_sub_accs_fixed)))\n",
    "                continue  \n",
    "\n",
    "            set_random_seeds(42)\n",
    "            loo_global_accs_fixed = []\n",
    "            loo_fine_tuning_accs_aug_fixed = []\n",
    "\n",
    "            for k in range(loo):\n",
    "                filterX, filterY = np.copy(X[..., :int(srate*duration)]), np.copy(y)\n",
    "                filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "\n",
    "                train_ind, validate_ind, test_ind = match_loo_indices(\n",
    "                    k, meta, indices)\n",
    "                trainX, trainY, trainMeta = filterX[train_ind], filterY[train_ind], meta.iloc[train_ind]\n",
    "                validateX, validateY, validateMeta = filterX[validate_ind], filterY[validate_ind], meta.iloc[validate_ind]\n",
    "                testX, testY, testMeta = filterX[test_ind], filterY[test_ind], meta.iloc[test_ind]\n",
    "\n",
    "                trainX, validateX, testX = generate_tensors(\n",
    "                    trainX, validateX, testX, dtype=x_dtype)\n",
    "                trainY, validateY, testY = generate_tensors(\n",
    "                    trainY, validateY, testY, dtype=y_dtype) \n",
    "\n",
    "                net = make_dl_model(\n",
    "                    model_name, n_channels, int(srate*duration), n_classes,\n",
    "                    Yf=Yf[..., :int(srate*duration)])\n",
    "                \n",
    "                lr, batch_size, max_epochs = generate_dl_parameters(\n",
    "                    dataset, model_name)\n",
    "                net.device = 'cpu'\n",
    "                net.lr = lr\n",
    "                net.max_epochs = max_epochs\n",
    "                net.batch_size = batch_size\n",
    "                net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "                net.verbose=False\n",
    "                net.initialize()\n",
    "\n",
    "                net.module.load_state_dict(\n",
    "                    copy.deepcopy(loo_global_model_states[k]))\n",
    "                \n",
    "                ## testing\n",
    "                sub_accs = []\n",
    "                for sub_id in dataset.subjects:\n",
    "                    # replace dtn templates with subject's grand average templates\n",
    "                    sub_train_mask = (trainMeta['subject']==sub_id).to_numpy()\n",
    "                    sub_trainX = (trainX[sub_train_mask]).numpy()\n",
    "                    sub_trainY = (trainY[sub_train_mask]).numpy()\n",
    "                    \n",
    "                    templates = np.stack(\n",
    "                        [np.mean(sub_trainX[sub_trainY==label], axis=0) for label in labels], 0)\n",
    "                    templates = torch.tensor(templates[:, np.newaxis, ...], dtype=torch.float)\n",
    "                    with torch.no_grad():\n",
    "                        net.module.eval()\n",
    "                        out = net.module.instance_norm(templates)\n",
    "                        out = net.module.feature_extractor(out)\n",
    "                        net.module.update_running_templates(out)\n",
    "                    sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "                    pred_labels = net.predict({'X': testX[sub_test_mask]})\n",
    "                    true_labels = testY[sub_test_mask].numpy()\n",
    "                    sub_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "                    sub_accs.append(sub_acc)\n",
    "                loo_global_accs_fixed.append(sub_accs)\n",
    "\n",
    "                ## fine-tuning\n",
    "                sub_accs_aug = []\n",
    "                for sub_id in dataset.subjects:\n",
    "                    sub_train_mask = (trainMeta['subject']==sub_id).to_numpy()\n",
    "                    sub_valid_mask = (validateMeta['subject']==sub_id).to_numpy()\n",
    "                    sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "\n",
    "                    sub_trainX, sub_trainY = trainX[sub_train_mask], trainY[sub_train_mask]\n",
    "                    sub_validateX, sub_validateY = validateX[sub_valid_mask], validateY[sub_valid_mask]\n",
    "                    sub_testX, sub_testY = testX[sub_test_mask], testY[sub_test_mask]    \n",
    "                    \n",
    "                    # augment training and validation data\n",
    "                    # the augmented data used for validation while the original training and validation data are combined for training\n",
    "                    sub_aug_trainX, sub_aug_trainY = generate_augment_data(\n",
    "                        np.concatenate([sub_trainX.numpy(), sub_validateX.numpy()], axis=0),\n",
    "                        np.concatenate([sub_trainY.numpy(), sub_validateY.numpy()], axis=0), aug_filterbank,\n",
    "                        n_trials_per_class=20)\n",
    "                    sub_aug_trainX = generate_tensors(sub_aug_trainX, dtype=x_dtype)\n",
    "                    sub_aug_trainY = generate_tensors(sub_aug_trainY, dtype=y_dtype)\n",
    "                    sub_trainX = torch.cat([sub_trainX, sub_validateX], 0)\n",
    "                    sub_trainY = torch.cat([sub_trainY, sub_validateY], 0) \n",
    "\n",
    "                    sub_validateX = sub_aug_trainX\n",
    "                    sub_validateY = sub_aug_trainY \n",
    "\n",
    "                    lr, batch_size, max_epochs = generate_dl_ft_parameters(\n",
    "                        dataset, model_name)\n",
    "\n",
    "                    net = make_dl_model(\n",
    "                        model_name, n_channels, int(srate*duration), n_classes,\n",
    "                        Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "                    net.device = device\n",
    "                    net.lr = lr\n",
    "                    net.max_epochs = max_epochs\n",
    "                    net.batch_size = batch_size\n",
    "                    net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "                    net.verbose=False\n",
    "                    net.module.load_state_dict(\n",
    "                        copy.deepcopy(loo_global_model_states[k]))\n",
    "\n",
    "                    net.train_split = predefined_split(\n",
    "                        skorch.dataset.Dataset(\n",
    "                            {'X': sub_validateX}, sub_validateY))\n",
    "\n",
    "                    if model_name == 'dtn':\n",
    "                        net = net.fit({'X': sub_trainX, 'y': sub_trainY}, y=sub_trainY)\n",
    "                    else:\n",
    "                        net = net.fit(\n",
    "                            {'X': sub_trainX}, \n",
    "                            y=sub_trainY)\n",
    "                    \n",
    "                    net.device = 'cpu'\n",
    "                    # replace dtn templates with subject's grand average templates\n",
    "                    sub_trainX = sub_trainX.numpy()\n",
    "                    sub_trainY = sub_trainY.numpy()\n",
    "                    templates = np.stack(\n",
    "                        [np.mean(sub_trainX[sub_trainY==label], axis=0) for label in labels], 0)\n",
    "                    templates = torch.tensor(templates[:, np.newaxis, ...], dtype=torch.float)\n",
    "                    with torch.no_grad():\n",
    "                        net.module.eval()\n",
    "                        net.module = net.module.to('cpu')\n",
    "                        out = net.module.instance_norm(templates)\n",
    "                        out = net.module.feature_extractor(out)\n",
    "                        net.module.update_running_templates(out)\n",
    "\n",
    "                    pred_labels = net.predict({'X': sub_testX})\n",
    "                    true_labels = sub_testY.numpy()\n",
    "                    sub_acc = balanced_accuracy_score(true_labels, pred_labels)\n",
    "                    sub_accs_aug.append(sub_acc)\n",
    "                loo_fine_tuning_accs_aug_fixed.append(sub_accs_aug)\n",
    "            global_sub_accs_fixed = np.array(loo_global_accs_fixed).T\n",
    "            ft_aug_sub_accs_fixed = np.array(loo_fine_tuning_accs_aug_fixed).T\n",
    "            joblib.dump(\n",
    "                {\n",
    "                    'global_sub_accs_fixed': global_sub_accs_fixed, \n",
    "                    'ft_aug_sub_accs_fixed': ft_aug_sub_accs_fixed\n",
    "                }, save_file)\n",
    "            print(\"{:s} acc:{:.4f} ft_aug_acc:{:.4f}\".format(\n",
    "                save_file, np.mean(global_sub_accs_fixed), np.mean(ft_aug_sub_accs_fixed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23479c-d128-4f9f-8205-23990caf2461",
   "metadata": {},
   "source": [
    " case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194d562e-88ad-41db-b2f9-d49033f766dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BETA()\n",
    "delay = 0.13\n",
    "dataset_channels = ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2']\n",
    "\n",
    "srate = 250\n",
    "duration = 0.5\n",
    "model_name = 'dtn'\n",
    "\n",
    "def data_hook(X, y, meta, caches):\n",
    "    filterbank = generate_filterbank([[8, 90]], [[6, 95]], srate, order=4, rp=1)\n",
    "    X = sosfiltfilt(filterbank[0], X, axis=-1)\n",
    "    return X, y, meta, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17c30a09-f3a5-46a8-98d7-b4d859ec710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "better = [8, 31, 43, 59]\n",
    "worse = [14, 25, 40, 56]\n",
    "subjects = better + worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2e5a72-6265-427e-bb70-64c6209cf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: beta Size: (11200, 9, 275)\n"
     ]
    }
   ],
   "source": [
    "dataset_events = sorted(list(dataset.events.keys()))\n",
    "freqs = [dataset.get_freq(event) for event in dataset_events]\n",
    "phases = [dataset.get_phase(event) for event in dataset_events]\n",
    "\n",
    "X, y, meta = get_ssvep_data(\n",
    "    dataset, srate, dataset_channels, 1.1, dataset_events, \n",
    "    delay=delay, \n",
    "    data_hook=data_hook)\n",
    "labels = np.unique(y)\n",
    "Yf = generate_cca_references(\n",
    "    freqs, srate, 1.1, \n",
    "    phases=None, \n",
    "    n_harmonics=n_harmonics)\n",
    "print(\"Dataset: {} Size: {}\".format(dataset.dataset_code, X.shape))\n",
    "_, n_channels, n_samples = X.shape\n",
    "n_classes = len(labels)\n",
    "\n",
    "indices = joblib.load(\n",
    "    \"indices/{:s}-loo-{:d}class-indices.joblib\".format(\n",
    "    dataset.dataset_code, n_classes))['indices']\n",
    "loo = len(indices[1][dataset_events[0]])\n",
    "\n",
    "min_f, max_f = np.min(freqs), np.max(freqs)\n",
    "wp = [[min_f*i, max_f*i] for i in range(1, n_harmonics+1)]\n",
    "ws= [[min_f*i-2, max_f*i+2] for i in range(1, n_harmonics+1)]\n",
    "aug_filterbank = generate_filterbank(wp, ws, srate, order=4, rp=1)\n",
    "\n",
    "file_name = make_file(\n",
    "    dataset, model_name, dataset_channels, srate, duration, dataset_events, \n",
    "    n_bands=n_bands)\n",
    "save_file = os.path.join(save_folder, file_name)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "loo_global_model_states = torch.load(\n",
    "    save_file.replace('.joblib', '.pt'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8483c013-3ffc-45c8-9508-e56881b3ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('sub_case_model_states.pt'):\n",
    "    loo_sub_model_states = []\n",
    "    for k in range(loo):\n",
    "        filterX, filterY = np.copy(X[..., :int(srate*duration)]), np.copy(y)\n",
    "        filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "\n",
    "        train_ind, validate_ind, test_ind = match_loo_indices(\n",
    "            k, meta, indices)\n",
    "        trainX, trainY, trainMeta = filterX[train_ind], filterY[train_ind], meta.iloc[train_ind]\n",
    "        validateX, validateY, validateMeta = filterX[validate_ind], filterY[validate_ind], meta.iloc[validate_ind]\n",
    "        testX, testY, testMeta = filterX[test_ind], filterY[test_ind], meta.iloc[test_ind]\n",
    "\n",
    "        trainX, validateX, testX = generate_tensors(\n",
    "            trainX, validateX, testX, dtype=x_dtype)\n",
    "        trainY, validateY, testY = generate_tensors(\n",
    "            trainY, validateY, testY, dtype=y_dtype) \n",
    "\n",
    "        ## fine-tuning\n",
    "        sub_model_states = []\n",
    "        for sub_id in subjects:\n",
    "            sub_train_mask = (trainMeta['subject']==sub_id).to_numpy()\n",
    "            sub_valid_mask = (validateMeta['subject']==sub_id).to_numpy()\n",
    "            sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "\n",
    "            sub_trainX, sub_trainY = trainX[sub_train_mask], trainY[sub_train_mask]\n",
    "            sub_validateX, sub_validateY = validateX[sub_valid_mask], validateY[sub_valid_mask]\n",
    "            sub_testX, sub_testY = testX[sub_test_mask], testY[sub_test_mask]    \n",
    "\n",
    "            # augment training and validation data\n",
    "            # the augmented data used for validation while the original training and validation data are combined for training\n",
    "            sub_aug_trainX, sub_aug_trainY = generate_augment_data(\n",
    "                np.concatenate([sub_trainX.numpy(), sub_validateX.numpy()], axis=0),\n",
    "                np.concatenate([sub_trainY.numpy(), sub_validateY.numpy()], axis=0), aug_filterbank,\n",
    "                n_trials_per_class=20)\n",
    "            sub_aug_trainX = generate_tensors(sub_aug_trainX, dtype=x_dtype)\n",
    "            sub_aug_trainY = generate_tensors(sub_aug_trainY, dtype=y_dtype)\n",
    "            sub_trainX = torch.cat([sub_trainX, sub_validateX], 0)\n",
    "            sub_trainY = torch.cat([sub_trainY, sub_validateY], 0) \n",
    "\n",
    "            sub_validateX = sub_aug_trainX\n",
    "            sub_validateY = sub_aug_trainY \n",
    "\n",
    "            lr, batch_size, max_epochs = generate_dl_ft_parameters(\n",
    "                dataset, model_name)\n",
    "\n",
    "            net = make_dl_model(\n",
    "                model_name, n_channels, int(srate*duration), n_classes,\n",
    "                Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "            net.device = device\n",
    "            net.lr = lr\n",
    "            net.max_epochs = max_epochs\n",
    "            net.batch_size = batch_size\n",
    "            net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "            net.verbose=False\n",
    "            net.module.load_state_dict(\n",
    "                copy.deepcopy(loo_global_model_states[k]))\n",
    "\n",
    "            net.train_split = predefined_split(\n",
    "                skorch.dataset.Dataset(\n",
    "                    {'X': sub_validateX}, sub_validateY))\n",
    "\n",
    "            if model_name == 'dtn':\n",
    "                net = net.fit({'X': sub_trainX, 'y': sub_trainY}, y=sub_trainY)\n",
    "            else:\n",
    "                net = net.fit(\n",
    "                    {'X': sub_trainX}, \n",
    "                    y=sub_trainY)\n",
    "\n",
    "            sub_model_states.append(\n",
    "                copy.deepcopy(net.module.state_dict()))\n",
    "        loo_sub_model_states.append(sub_model_states)\n",
    "    loo_sub_model_states = [x for x in zip(*loo_sub_model_states)]\n",
    "    torch.save(loo_sub_model_states, 'sub_case_model_states.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01cc364f-7e07-4f97-8598-0e702d0295f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_sub_model_states = torch.load(\n",
    "    'sub_case_model_states.pt', map_location='cpu')\n",
    "\n",
    "net = make_dl_model(\n",
    "    'dtn', n_channels, int(srate*duration), n_classes,\n",
    "    Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "dtn_sub_templates1 = []\n",
    "for i_sub in range(len(subjects)):\n",
    "    template = 0 \n",
    "    for k in range(loo):\n",
    "        net.module.load_state_dict(\n",
    "            copy.deepcopy(loo_sub_model_states[i_sub][k]))\n",
    "        template += net.module.running_template.detach().numpy()\n",
    "    template /= loo\n",
    "    dtn_sub_templates1.append(np.squeeze(template))\n",
    "\n",
    "filterX, filterY = np.copy(X[..., :int(srate*duration)]), np.copy(y)\n",
    "filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "\n",
    "dtn_sub_templates2 = []\n",
    "for i_sub in range(len(subjects)):\n",
    "    subX, subY = filterX[meta['subject']==subjects[i_sub]], filterY[meta['subject']==subjects[i_sub]]\n",
    "    templates = []\n",
    "    for label in labels:\n",
    "        templates.append(np.mean(subX[subY==label], axis=0))\n",
    "    templates = np.stack(templates, 0)\n",
    "    templates = torch.tensor(templates[:, np.newaxis, ...], dtype=torch.float)\n",
    "    \n",
    "    template = 0 \n",
    "    for k in range(loo):\n",
    "        net.module.load_state_dict(\n",
    "            copy.deepcopy(loo_sub_model_states[i_sub][k]))\n",
    "        net.module.eval()\n",
    "        out = net.module.instance_norm(templates)\n",
    "        out = net.module.feature_extractor(out).detach().numpy()\n",
    "        template += out\n",
    "    template /= loo\n",
    "    dtn_sub_templates2.append(np.squeeze(template))\n",
    "\n",
    "joblib.dump(\n",
    "    {'dtn_sub_templates1': dtn_sub_templates1, 'dtn_sub_templates2': dtn_sub_templates2},\n",
    "    'sub_templates.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f8502-9104-4a7c-9002-04054cd5668b",
   "metadata": {},
   "source": [
    "computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19fd8c9b-3204-42d2-86bb-cb2c42e6d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BETA()\n",
    "delay = 0.13\n",
    "dataset_channels = ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2']\n",
    "\n",
    "srate = 250\n",
    "duration = 0.5\n",
    "model_name = 'ftn'\n",
    "\n",
    "def data_hook(X, y, meta, caches):\n",
    "    filterbank = generate_filterbank([[8, 90]], [[6, 95]], srate, order=4, rp=1)\n",
    "    X = sosfiltfilt(filterbank[0], X, axis=-1)\n",
    "    return X, y, meta, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2a56f1c-81f6-43ee-9418-c7ce1a468938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: beta Size: (11200, 9, 275)\n"
     ]
    }
   ],
   "source": [
    "dataset_events = sorted(list(dataset.events.keys()))\n",
    "freqs = [dataset.get_freq(event) for event in dataset_events]\n",
    "phases = [dataset.get_phase(event) for event in dataset_events]\n",
    "\n",
    "X, y, meta = get_ssvep_data(\n",
    "    dataset, srate, dataset_channels, 1.1, dataset_events, \n",
    "    delay=delay, \n",
    "    data_hook=data_hook)\n",
    "labels = np.unique(y)\n",
    "Yf = generate_cca_references(\n",
    "    freqs, srate, 1.1, \n",
    "    phases=None, \n",
    "    n_harmonics=n_harmonics)\n",
    "print(\"Dataset: {} Size: {}\".format(dataset.dataset_code, X.shape))\n",
    "_, n_channels, n_samples = X.shape\n",
    "n_classes = len(labels)\n",
    "\n",
    "indices = joblib.load(\n",
    "    \"indices/{:s}-loo-{:d}class-indices.joblib\".format(\n",
    "    dataset.dataset_code, n_classes))['indices']\n",
    "loo = len(indices[1][dataset_events[0]])\n",
    "\n",
    "min_f, max_f = np.min(freqs), np.max(freqs)\n",
    "wp = [[min_f*i, max_f*i] for i in range(1, n_harmonics+1)]\n",
    "ws= [[min_f*i-2, max_f*i+2] for i in range(1, n_harmonics+1)]\n",
    "aug_filterbank = generate_filterbank(wp, ws, srate, order=4, rp=1)\n",
    "\n",
    "file_name = make_file(\n",
    "    dataset, model_name, dataset_channels, srate, duration, dataset_events, \n",
    "    n_bands=n_bands)\n",
    "save_file = os.path.join(save_folder, file_name)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "loo_global_model_states = torch.load(\n",
    "    save_file.replace('.joblib', '.pt'), map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed15ee84-7676-43ab-9152-b93243decf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = 0 #gpu\n",
    "inference_time = 0 #cpu\n",
    "for k in range(loo):\n",
    "    filterX, filterY = np.copy(X[..., :int(srate*duration)]), np.copy(y)\n",
    "    filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "\n",
    "    train_ind, validate_ind, test_ind = match_loo_indices(\n",
    "        k, meta, indices)\n",
    "    trainX, trainY, trainMeta = filterX[train_ind], filterY[train_ind], meta.iloc[train_ind]\n",
    "    validateX, validateY, validateMeta = filterX[validate_ind], filterY[validate_ind], meta.iloc[validate_ind]\n",
    "    testX, testY, testMeta = filterX[test_ind], filterY[test_ind], meta.iloc[test_ind]\n",
    "\n",
    "    trainX, validateX, testX = generate_tensors(\n",
    "        trainX, validateX, testX, dtype=x_dtype)\n",
    "    trainY, validateY, testY = generate_tensors(\n",
    "        trainY, validateY, testY, dtype=y_dtype) \n",
    "\n",
    "    ## fine-tuning\n",
    "    for sub_id in [1]:\n",
    "        sub_train_mask = (trainMeta['subject']==sub_id).to_numpy()\n",
    "        sub_valid_mask = (validateMeta['subject']==sub_id).to_numpy()\n",
    "        sub_test_mask = (testMeta['subject']==sub_id).to_numpy()\n",
    "\n",
    "        sub_trainX, sub_trainY = trainX[sub_train_mask], trainY[sub_train_mask]\n",
    "        sub_validateX, sub_validateY = validateX[sub_valid_mask], validateY[sub_valid_mask]\n",
    "        sub_testX, sub_testY = testX[sub_test_mask], testY[sub_test_mask]    \n",
    "\n",
    "        # augment training and validation data\n",
    "        # the augmented data used for validation while the original training and validation data are combined for training\n",
    "        sub_aug_trainX, sub_aug_trainY = generate_augment_data(\n",
    "            np.concatenate([sub_trainX.numpy(), sub_validateX.numpy()], axis=0),\n",
    "            np.concatenate([sub_trainY.numpy(), sub_validateY.numpy()], axis=0), aug_filterbank,\n",
    "            n_trials_per_class=20)\n",
    "        sub_aug_trainX = generate_tensors(sub_aug_trainX, dtype=x_dtype)\n",
    "        sub_aug_trainY = generate_tensors(sub_aug_trainY, dtype=y_dtype)\n",
    "        sub_trainX = torch.cat([sub_trainX, sub_validateX], 0)\n",
    "        sub_trainY = torch.cat([sub_trainY, sub_validateY], 0) \n",
    "\n",
    "        sub_validateX = sub_aug_trainX\n",
    "        sub_validateY = sub_aug_trainY \n",
    "\n",
    "        lr, batch_size, max_epochs = generate_dl_ft_parameters(\n",
    "            dataset, model_name)\n",
    "\n",
    "        net = make_dl_model(\n",
    "            model_name, n_channels, int(srate*duration), n_classes,\n",
    "            Yf=Yf[..., :int(srate*duration)])\n",
    "\n",
    "        net.device = device\n",
    "        net.lr = lr\n",
    "        net.max_epochs = max_epochs\n",
    "        net.batch_size = batch_size\n",
    "        net.set_params(callbacks__lr_scheduler__T_max=max_epochs-1)\n",
    "        net.verbose=False\n",
    "        net.module.load_state_dict(\n",
    "            copy.deepcopy(loo_global_model_states[k]))\n",
    "\n",
    "        net.train_split = predefined_split(\n",
    "            skorch.dataset.Dataset(\n",
    "                {'X': sub_validateX}, sub_validateY))\n",
    "        \n",
    "        start_t = time.time()\n",
    "        if model_name == 'dtn':\n",
    "            net = net.fit({'X': sub_trainX, 'y': sub_trainY}, y=sub_trainY)\n",
    "        else:\n",
    "            net = net.fit(\n",
    "                {'X': sub_trainX}, \n",
    "                y=sub_trainY)\n",
    "        end_t = time.time()\n",
    "        training_time += (end_t-start_t)\n",
    "        \n",
    "        module = net.module\n",
    "        module.eval()\n",
    "        module = module.to('cpu')\n",
    "        \n",
    "        tmp = 0\n",
    "        for i in range(len(sub_testX)):\n",
    "            start_t = time.time()\n",
    "            pred_label = torch.argmax(module(sub_testX[i][np.newaxis, ...]), -1)\n",
    "            end_t = time.time()\n",
    "            tmp += (end_t -start_t)\n",
    "        tmp /= len(sub_testX)\n",
    "        inference_time += tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f26d7fd-5e13-47a3-ba67-0ffd367c5093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training time: 22.9188s\n",
      "average inference time: 0.0107s\n"
     ]
    }
   ],
   "source": [
    "training_time /= loo\n",
    "inference_time /= loo\n",
    "print(\"average training time: {:.4f}s\".format(training_time))\n",
    "print(\"average inference time: {:.4f}s\".format(inference_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
